{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittfgpuconda4815d6332dd1431691508f462cd4b2ad",
   "display_name": "Python 3.7.6 64-bit ('tf-gpu': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3 Tweet Generation\n",
    "Ya se hizo la limpieza de datos inicial de este dataset en el notebook [Analisis Dataset 2](00_2_AnalisisDataset2.ipynb)\n",
    "\n",
    "En este notebook generaremos tweets haciendo que se parezcan a los que twitea Trump, para ello, usaremos Language Modeling con Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (2.2.5)\nRequirement already satisfied: spacy>=2.2.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.3)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\nRequirement already satisfied: setuptools in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.1.0.post20200127)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\nRequirement already satisfied: thinc<7.4.0,>=7.3.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\nRequirement already satisfied: srsly<1.1.0,>=0.1.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.1)\nRequirement already satisfied: numpy>=1.15.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\nRequirement already satisfied: idna<2.9,>=2.5 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.42.1)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\nRequirement already satisfied: zipp>=0.5 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.1.0)\n✔ Download and installation successful\nYou can now load the model via spacy.load('en_core_web_sm')\nc:\\Users\\asens\\.vscode\\extensions\\ms-python.python-2020.2.64397\\pythonFiles\\lib\\python\\past\\types\\oldstr.py:36: DeprecationWarning: invalid escape sequence \\d\n  \"\"\"\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nUsing TensorFlow backend.\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\learn_io\\generator_io.py:26: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import Container\n"
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import io\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import pylab as pl\n",
    "from random import shuffle, choice, sample\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from copy import copy\n",
    "from IPython import display\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dot, Concatenate, Flatten, Permute, Multiply, dot, concatenate\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "\n",
    "# Comando nvidia-smi para ver la lista de GPU\n",
    "\n",
    "set_session(tf.Session(config=config))  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permitir ver columnas completas\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Función para cargar dataframe\n",
    "def load_data(ruta, nombre):\n",
    "    path = os.path.join(ruta, nombre)    \n",
    "    return pd.read_csv(path, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>president</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>OBAMA</td>\n      <td>low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OBAMA</td>\n      <td>low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>OBAMA</td>\n      <td>national gun violence awareness day show commitment keep kid safe gun violence everyone weve lose take action change leadership laws reflect commitment matter long take</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>OBAMA</td>\n      <td>never truly debt owe fall heroes remember honor sacrifice live endure ideals justice equality oppounity generations americans give last full measure devotion</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>OBAMA</td>\n      <td>center leaders tomorrow ready step build world michelle grateful chicago city council make happen</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  president  \\\n0     OBAMA   \n1     OBAMA   \n2     OBAMA   \n3     OBAMA   \n4     OBAMA   \n\n                                                                                                                                                                        tweet  \n0  low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss  \n1  low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss  \n2    national gun violence awareness day show commitment keep kid safe gun violence everyone weve lose take action change leadership laws reflect commitment matter long take  \n3               never truly debt owe fall heroes remember honor sacrifice live endure ideals justice equality oppounity generations americans give last full measure devotion  \n4                                                                           center leaders tomorrow ready step build world michelle grateful chicago city council make happen  "
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "df = load_data('data', 'dataset_2_limpieza_inicial.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionamos los tweets de Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>president</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2892</th>\n      <td>TRUMP</td>\n      <td>thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes</td>\n    </tr>\n    <tr>\n      <th>2893</th>\n      <td>TRUMP</td>\n      <td>thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes</td>\n    </tr>\n    <tr>\n      <th>2894</th>\n      <td>TRUMP</td>\n      <td>head canada grinÃ talk mostly center long time unfair trade unite state go singapore talk noh korea denuclearization wont talk russian witch hunt hoax</td>\n    </tr>\n    <tr>\n      <th>2895</th>\n      <td>TRUMP</td>\n      <td>congratulations washington capitals great play win cup championship alex team spectacular true dc many ways time</td>\n    </tr>\n    <tr>\n      <th>2896</th>\n      <td>TRUMP</td>\n      <td>look forward unfair trade deal grinÃ countries doesnt happen come even better</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5874</th>\n      <td>TRUMP</td>\n      <td>vast money nato amp unite state must pay powerful expensive defense provide germany</td>\n    </tr>\n    <tr>\n      <th>5875</th>\n      <td>TRUMP</td>\n      <td>despite hear fake news great meet chancellor angela merkel germany</td>\n    </tr>\n    <tr>\n      <th>5876</th>\n      <td>TRUMP</td>\n      <td>great meet committee morning</td>\n    </tr>\n    <tr>\n      <th>5877</th>\n      <td>TRUMP</td>\n      <td>president change small confidence</td>\n    </tr>\n    <tr>\n      <th>5878</th>\n      <td>TRUMP</td>\n      <td>noh korea behave badly play unite state years china do little help</td>\n    </tr>\n  </tbody>\n</table>\n<p>2987 rows × 2 columns</p>\n</div>",
      "text/plain": "     president  \\\n2892     TRUMP   \n2893     TRUMP   \n2894     TRUMP   \n2895     TRUMP   \n2896     TRUMP   \n...        ...   \n5874     TRUMP   \n5875     TRUMP   \n5876     TRUMP   \n5877     TRUMP   \n5878     TRUMP   \n\n                                                                                                                                                        tweet  \n2892                                                                   thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes  \n2893                                                                   thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes  \n2894  head canada grinÃ talk mostly center long time unfair trade unite state go singapore talk noh korea denuclearization wont talk russian witch hunt hoax  \n2895                                         congratulations washington capitals great play win cup championship alex team spectacular true dc many ways time  \n2896                                                                           look forward unfair trade deal grinÃ countries doesnt happen come even better  \n...                                                                                                                                                       ...  \n5874                                                                      vast money nato amp unite state must pay powerful expensive defense provide germany  \n5875                                                                                       despite hear fake news great meet chancellor angela merkel germany  \n5876                                                                                                                             great meet committee morning  \n5877                                                                                                                        president change small confidence  \n5878                                                                                       noh korea behave badly play unite state years china do little help  \n\n[2987 rows x 2 columns]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['president'] == 'TRUMP']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2987,\n 'thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_dataset = list()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row[1]\n",
    "    trump_dataset.append(sentence)\n",
    "\n",
    "len(trump_dataset), trump_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar a nivel de carácter en lugar de palaba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [list(x) for x in trump_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['<SOS>', 't', 'h', 'o', 'u']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_chars = [x[:5] for x in tokenized] # me quedo con los 4 caracteres iniciales\n",
    "\n",
    "for i in range(len(init_chars)):\n",
    "    tmp = init_chars[i]\n",
    "    tmp.insert(0, '<SOS>')\n",
    "    init_chars[i] = tmp[:5]\n",
    "    \n",
    "init_chars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengo la longitud máxima de las frases y la longitud media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Longitud maxima:  219\nMedia de Longitud:  89.00836960160696\n"
    }
   ],
   "source": [
    "maxlen = max([len(x) for x in tokenized])\n",
    "avglen = sum([len(x) for x in tokenized])/len(tokenized)\n",
    "print('Longitud maxima: ', maxlen)\n",
    "print('Media de Longitud: ', avglen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Numero de tokens: 265868\n"
    }
   ],
   "source": [
    "total_tokens = [t for s in trump_dataset for t in s]\n",
    "print('Numero de tokens: {}'.format(len(total_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "34"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab_counter = Counter(total_tokens)\n",
    "vocab = [w for w, v in vocab_counter.items() if v > 2]  # Caracteres que mínimo aparezcan 2 veces\n",
    "vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + vocab\n",
    "\n",
    "nb_vocab = len(vocab)\n",
    "nb_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2id = {k:i for i, k in enumerate(vocab)}\n",
    "id2w = {i:k for k, i in c2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "256907"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 5\n",
    "\n",
    "step = 1\n",
    "\n",
    "data_train = []\n",
    "\n",
    "for x in tokenized:\n",
    "    x.insert(0, '<SOS>')\n",
    "    x.append('<EOS>')\n",
    "    for i in range(0, len(x)-maxlen, step):\n",
    "        data_train.append((x[i:i+maxlen], x[i+maxlen]))\n",
    "        \n",
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_EVERY = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pred(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampletest(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % SAMPLE_EVERY == 0  and epoch>0:\n",
    "            data_test = []\n",
    "            nb_samples = 1\n",
    "            \n",
    "            params = {\n",
    "                'maxlen': maxlen,\n",
    "                'vocab': nb_vocab,\n",
    "                'use_embeddings': True\n",
    "                }\n",
    "            for _ in range(nb_samples):\n",
    "                data_test = choice(init_chars)\n",
    "                for diversity in [0.2, 0.6, 1.2]:\n",
    "                    print('----- diversity:', diversity)\n",
    "                    sentence = copy(data_test)\n",
    "                    generated = copy(data_test)\n",
    "                    for i in range(len(data_test), 400):\n",
    "                        x_pred = np.zeros((1, params['maxlen']))\n",
    "                        for t, char in enumerate(sentence):\n",
    "                            x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
    "                        preds = self.model.predict(x_pred, verbose=0)[0]\n",
    "                        next_index = sample_pred(preds, diversity)\n",
    "                        next_char = id2w[next_index]\n",
    "                        if next_char == '<EOS>':\n",
    "                            break\n",
    "                        generated += [next_char]\n",
    "                        sentence = sentence[1:] \n",
    "                        sentence += [next_char]\n",
    "                    print(''.join(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDisplay(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.epochs = []\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        #plt.show()\n",
    "        \n",
    "        plt.ion()\n",
    "        self.fig.show()\n",
    "        self.fig.canvas.draw()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.epochs.append(epoch)\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.accs.append(logs['acc'])\n",
    "        if epoch % PLOT_EVERY == 0:\n",
    "            \n",
    "            self.ax.clear()\n",
    "            self.ax.plot(self.epochs, self.accs, 'g', label='acc')\n",
    "            self.ax.plot(self.epochs, self.losses, 'b', label='loss')\n",
    "            legend = self.ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n",
    "            #display.clear_output(wait=True)\n",
    "            #display.display(pl.gcf())\n",
    "            self.fig.canvas.draw()\n",
    "            \n",
    "            #plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decidir arquitectura y preparar el train y el predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs.pop('params', None)\n",
    "    \n",
    "    def compile_bidirectional(self, params={}):\n",
    "        lm_inputs = Input(shape=(params['maxlen'], ), name='lm_input')\n",
    "        embeddings = Embedding(params['vocab'], params['emb_feats'])(lm_inputs)\n",
    "        lstm =  CuDNNLSTM(params['rnn_hidden'], return_sequences=True, name='rnn1')        \n",
    "        \n",
    "        lmlstm = Bidirectional(lstm)(embeddings)       \n",
    "        \n",
    "        stacklstm =  CuDNNLSTM(params['rnn_hidden'], return_sequences=False, name='stacked')\n",
    "        stackedlstm = stacklstm(lmlstm)\n",
    "        \n",
    "        lmout = Dense(params['vocab'], activation='softmax')(stackedlstm)\n",
    "        \n",
    "        model = Model(lm_inputs, lmout)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='rmsprop', \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, model, data, params={}):\n",
    "        callbacks = self._get_callbacks()\n",
    "        \n",
    "        if 'shuffle' in params and params['shuffle']:\n",
    "            shuffle(data)\n",
    "            \n",
    "        sentences, next_chars = zip(*data)\n",
    "        #print(sentences[0])\n",
    "\n",
    "        x = np.zeros((len(data), params['maxlen']))\n",
    "        y = np.zeros((len(data), params['vocab']))\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[i, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
    "            y[i, c2id[next_chars[i]] if next_chars[i] in c2id else c2id['<UNK>']]  = 1\n",
    "        \n",
    "        model.fit(x, y, batch_size=params['batch_size'], epochs=params['epochs'], callbacks=callbacks, verbose=1)\n",
    "\n",
    "    def predict(self, model, data, params={}):        \n",
    "        if 'use_embeddings' in params and params['use_embeddings']:\n",
    "            # variedad en las predicciones\n",
    "            for diversity in [0.2, 0.6, 1.2]:\n",
    "                print('----- diversity:', diversity)\n",
    "                sentence = copy(data)\n",
    "                generated = copy(data)\n",
    "                # cuantas predicciones queremos hacer\n",
    "                for i in range(len(data), 400):\n",
    "                    x_pred = np.zeros((1, params['maxlen']))\n",
    "                    # preparar inpunt\n",
    "                    for t, char in enumerate(sentence):\n",
    "                        x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
    "                    # predecir\n",
    "                    preds = self.model.predict(x_pred, verbose=0)[0]\n",
    "                    next_index = sample_pred(preds, diversity)\n",
    "                    next_char = id2w[next_index]\n",
    "                    # mirar si hemos terminado\n",
    "                    if next_char == '<EOS>':\n",
    "                        break\n",
    "                    # ana                        \n",
    "                    generated += [next_char]\n",
    "                    sentence = sentence[1:] \n",
    "                    sentence += [next_char]\n",
    "                print(''.join(generated))\n",
    "    \n",
    "    \n",
    "    def load(self, model_path='seq2seq_attn.h5'):\n",
    "        return load_model(model_path)\n",
    "    \n",
    "    def _get_callbacks(self, model_path='seq2seq_attn.h5'):\n",
    "        es = EarlyStopping(monitor='loss', patience=4, mode='auto', verbose=0)       \n",
    "        \n",
    "        save_best = ModelCheckpoint(model_path, monitor='loss', verbose = 0, save_best_only=True, save_weights_only=False, period=2)\n",
    "        st = Sampletest()\n",
    "        # hd = HistoryDisplay()\n",
    "        rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
    "        return [st, rlr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_params = {\n",
    "    'maxlen': maxlen, \n",
    "    'vocab': len(vocab),\n",
    "    'emb_feats': 100,\n",
    "    'rnn_hidden': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlm_input (InputLayer)        (None, 5)                 0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 5, 100)            3400      \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 5, 512)            733184    \n_________________________________________________________________\nstacked (CuDNNLSTM)          (None, 256)               788480    \n_________________________________________________________________\ndense_2 (Dense)              (None, 34)                8738      \n=================================================================\nTotal params: 1,533,802\nTrainable params: 1,533,802\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "lm = LM()\n",
    "lm_model = lm.compile_bidirectional(params=compile_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " hard meet change make american people amp stock market hit another make america great honor welcome president trump campaign proud state terrorist attack london strong well\n----- diversity: 0.6\n<SOS>history\n----- diversity: 1.2\n<SOS>history amp begin repeal map thank give news away service charge clinton morning\nEpoch 210/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7061 - acc: 0.7290\nEpoch 211/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7061 - acc: 0.7288\nEpoch 212/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7061 - acc: 0.7292\nEpoch 213/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7060 - acc: 0.7291\n----- diversity: 0.2\n<SOS>fake news media work hard believe fake news media cover amp deductibles way get read schedule time high would shape devin mccabe clinton campaign problem lie show collusion one years talk action people want take place today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>fake news win republicans send bad people fact fbi brennan dishonesty amp replace obamacare place long improve fake news mediately never include penalty\n----- diversity: 1.2\n<SOS>fake news bias release join maga\nEpoch 214/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7060 - acc: 0.7288\nEpoch 215/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7060 - acc: 0.7290\nEpoch 216/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7060 - acc: 0.7288\nEpoch 217/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7060 - acc: 0.7285\n----- diversity: 0.2\n<SOS>one th thoughts prayers stand nation come back usa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>one years get lower president rate company clinton indianapolis maria b\n----- diversity: 1.2\n<SOS>one accurate beat solve process im prepare anythings back day unite state thing need us two days greatagain\nEpoch 218/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7060 - acc: 0.7289\nEpoch 219/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7060 - acc: 0.7290\nEpoch 220/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7059 - acc: 0.7290\nEpoch 221/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7059 - acc: 0.7291\n----- diversity: 0.2\n<SOS>supreme country state trump campaign problem little time secretary state american pay respect country work close election dollars state terrorist attack london strong well\n----- diversity: 0.6\n<SOS>supreme country cannot stand nation repeal amp london strongly congress must end chain migration company announce november stand need people dont world leader help us alconv\n----- diversity: 1.2\n<SOS>supreme close hundreds thousands energy include illegally believe national another take\nEpoch 222/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7059 - acc: 0.7291\nEpoch 223/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7059 - acc: 0.7292\nEpoch 224/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7059 - acc: 0.7293\nEpoch 225/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7059 - acc: 0.7291\n----- diversity: 0.2\n<SOS>terrorist attack london strong well\n----- diversity: 0.6\n<SOS>terrorist attack london strong well judicial country state peng like watch hunt give us honor welcome president unite state terrorist attack london strong well\n----- diversity: 1.2\n<SOS>terrorist attack london strong forget\nEpoch 226/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7058 - acc: 0.7293\nEpoch 227/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7058 - acc: 0.7291\nEpoch 228/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7058 - acc: 0.7292\nEpoch 229/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7058 - acc: 0.7287\n----- diversity: 0.2\n<SOS>new healthcare president make american people well\n----- diversity: 0.6\n<SOS>new history executive order democrats want take media want take care proclaim october\n----- diversity: 1.2\n<SOS>new healthcare da\nEpoch 230/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7289\nEpoch 231/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7058 - acc: 0.7290\nEpoch 232/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7058 - acc: 0.7288\nEpoch 233/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7292\n----- diversity: 0.2\n<SOS>democrats want state american senator rich build wall strengthen cnn abc news media cover amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>democrats deserve country taxreform bill\n----- diversity: 1.2\n<SOS>democrats bipaisan meddle run another form billions people take official meet since effos spend billions restoration healthcare enthusiastic crowd spectful candidate officially wrong st charge thing promise\nEpoch 234/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7291\nEpoch 235/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7290\nEpoch 236/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7293\nEpoch 237/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7291\n----- diversity: 0.2\n<SOS>visit watch confidence company come country state terrorist attack london strong well\n----- diversity: 0.6\n<SOS>visit president last years talk nomination president make american publicans house security country come presidential meet kim jong un noh korea act stop people state set read\n----- diversity: 1.2\n<SOS>visit deserver\nEpoch 238/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7289\nEpoch 239/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7292\nEpoch 240/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7057 - acc: 0.7288\nEpoch 241/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7056 - acc: 0.7289\n----- diversity: 0.2\n<SOS>senator luther great state trump campaign proud know something everyone th th rapidly carry prove along well\n----- diversity: 0.6\n<SOS>senate dems continue heroes put life safety country billions full really big help people families bad country flag amp dnc rig amp tax cut american people law enforcement americane effective order security border amp border president donald j trump replace healthcare\n----- diversity: 1.2\n<SOS>senator sign dnc refuse past year'\nEpoch 242/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7056 - acc: 0.7289\nEpoch 243/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7056 - acc: 0.7291\nEpoch 244/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7056 - acc: 0.7288\nEpoch 245/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7056 - acc: 0.7293\n----- diversity: 0.2\n<SOS>great honor welcome back work hard believe fake news media control committee modern tax cut reform billion dollars senate look forward meet president moon south korea success fact country much better condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>great new york tirelessly believe bannon see whether make great team standard afternoon democrats dossier lie confidency expand great people state pass bill nosanctuary clinton foune charge us become together patriot joe dignity allow happy announce november say thoughts prayers business make show ball time take thank state get would win wonderful state office sacrifice make american people great\n----- diversity: 1.2\n<SOS>great border will amp amp yet fact\nEpoch 246/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7056 - acc: 0.7288\nEpoch 247/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7056 - acc: 0.7290\nEpoch 248/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7288\nEpoch 249/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7291\n----- diversity: 0.2\n<SOS>look forward meet president trump campaign problem little time amp replace committee modern time state trump campaign collusion russia coverage many bill amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>look forward must field trump action do little constitutions still military story\n----- diversity: 1.2\n<SOS>look ads go run th anniversary amp vote cover forget make advantage last year\nEpoch 250/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7292\nEpoch 251/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7289\nEpoch 252/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7286\nEpoch 253/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7288\n----- diversity: 0.2\n<SOS>texas amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>texas close alabama white house interest witch hunt privilege whole despite dems thugs hit people congress great job job\n----- diversity: 1.2\n<SOS>texas early yester nj longer date texas louily win house concern actually home meet success tax weak bad star eye rampant victory phony without thing\nEpoch 254/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7289\nEpoch 255/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7287\nEpoch 256/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7055 - acc: 0.7288\nEpoch 257/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7288\n----- diversity: 0.2\n<SOS>one soon south korea state terrorist attack london strong well\n----- diversity: 0.6\n<SOS>one th july us fail country usa\n----- diversity: 1.2\n<SOS>one year donald j trump obstruction noko region innovators wonderful talk repeal amp violences term accord lose leave sanderinchief keect plane credible women's past two dozen yesterday life must story thank president obstructure dream medicinisover\nEpoch 258/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7284\nEpoch 259/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7286\nEpoch 260/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7288\nEpoch 261/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7287\n----- diversity: 0.2\n<SOS>great honor welcome president xi jinping come president trump campaign collusion reform bill amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>great honor welcome never security today offer control\n----- diversity: 1.2\n<SOS>great veterans democrat meet suppoint charlottesville read rate unbelieve team comeyer distinguish delete\nEpoch 262/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7288\nEpoch 263/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7290\nEpoch 264/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7291\nEpoch 265/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7289\n----- diversity: 0.2\n<SOS>many years talk noh korea make american people country state amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>many decision strong well\n----- diversity: 1.2\n<SOS>many people\nEpoch 266/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7288\nEpoch 267/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7054 - acc: 0.7286\nEpoch 268/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7289\nEpoch 269/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7289\n----- diversity: 0.2\n<SOS>welcome president obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>welcome prime ministrations dollars since state class job job act years want president try hide\n----- diversity: 1.2\n<SOS>welcome prevent unite star\nEpoch 270/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7288\nEpoch 271/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7291\nEpoch 272/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7286\nEpoch 273/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7288\n----- diversity: 0.2\n<SOS>much better strange great honor welcome prime minister theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>much stand come president democrat vote senate american people alabama cant let school bad thing protect could even cannot come president tell republicans even live people include fox news wrong well tax cut bill amp replace like care put amp also noh korea launch action strong well years\n----- diversity: 1.2\n<SOS>much tough laws immigrations\nEpoch 274/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7288\nEpoch 275/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7288\nEpoch 276/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7287\nEpoch 277/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7288\n----- diversity: 0.2\n<SOS>big win election dollars spend thank great state many bill military state democrats want stop drug people country state amp thank great honor sign today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>big win general jackson one years us fail new york time take place depament aircraft letter clinton strong well\n----- diversity: 1.2\n<SOS>big luther join live email\nEpoch 278/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7287\nEpoch 279/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7053 - acc: 0.7286\nEpoch 280/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7289\nEpoch 281/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7287\n----- diversity: 0.2\n<SOS>cut tax cut reform bill pass big deal democrats want stop drug pour back usa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>cut tax cut american job president putin must end unfair trade china tariff pay leave fl teachers\n----- diversity: 1.2\n<SOS>cut thing pennsylvania oppose california white house many way alabama administer abc amp goal\nEpoch 282/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7291\nEpoch 283/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7289\nEpoch 284/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7290\nEpoch 285/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7289\n----- diversity: 0.2\n<SOS>great honor sign trouble see great people work hard meet president obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>great military staed wall\n----- diversity: 1.2\n<SOS>great job job job president nation transcript\nEpoch 286/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7290\nEpoch 287/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7285\nEpoch 288/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7288\nEpoch 289/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7287\n----- diversity: 0.2\n<SOS>arrive information trump campaign problem like company amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>arrive company big success never relation us could suppoers eve jam comey leaders shoot sick grinÃ canada make plan research thoughts problem next years hard believe fake news cnn dishonest day yrs since election dollars years last night back congress must stop massive trade deal include ambassadors nothing get victory country worst full record stock market unfair trade deal mandate long two day\n----- diversity: 1.2\n<SOS>arrive witch mcconnell south korean register rasmussen policy worse noh korea must sadly able many fantastic health country great regulation day offer college greate must\nEpoch 290/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7052 - acc: 0.7281\nEpoch 291/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7285\nEpoch 292/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7288\nEpoch 293/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7289\n----- diversity: 0.2\n<SOS>congress make american pay republicans stand paul mandate go along time highest level play field american people amp corrupt mainstream media want state amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>congress amp bad country right amazon must pa negotiate trump approval rat alike many time strong well\n----- diversity: 1.2\n<SOS>congress piece begin end love political cemetery late problem dont wrong oppose count place great day number weapons expensive trail perhaps china sgate\nEpoch 294/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7286\nEpoch 295/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7288\nEpoch 296/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7286\nEpoch 297/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7289\n----- diversity: 0.2\n<SOS>shoplift let country record high crime minister theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.6\n<SOS>shoplift big deal great fact stop interview stand\n----- diversity: 1.2\n<SOS>shoplift say collude us laugh didnt problem next badly want us women's us future politics insurance billions dollars president foreign russia investment obamacare military fire fbi look forward sign land jump simple great border rest use december\nEpoch 298/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7290\nEpoch 299/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7290\nEpoch 300/300\n256907/256907 [==============================] - 12s 46us/step - loss: 0.7051 - acc: 0.7289\n"
    }
   ],
   "source": [
    "train_params = {\n",
    "    'epochs': 300,\n",
    "    'batch_size': 512,\n",
    "    'shuffle': True,\n",
    "    'vocab': nb_vocab,\n",
    "    'maxlen': maxlen,\n",
    "    'use_embeddings': True\n",
    "}\n",
    "\n",
    "lm.train(model=lm_model, data=data_train, params=train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "Por algún motivo se ha borrado el log anterior a la época 210, no tengo tiempo de volver a generarlo.\n",
    "\n",
    "Si miramos los datos que genera el entrenamiento, comprobamos que los tweets generados con diversidad mayor a 0.2 dejan de tener sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testear el modelo\n",
    "Probamos con diversidad 0.2 a generar 5 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "----- diversity: 0.2\npoor economy boom get today offer condolences terrorist attack london strong well\n----- diversity: 0.2\ngeneral meet president trump campaign proud state congress make america great honor welcome amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.2\nspect country must always president money spend military state america great honor sign trillion dollars see saudi arabia peace memos clinton investigation best tax cut american people great meet president xi china already protect country state trump campaign problem didnt want state terrorist attack london strong well\n----- diversity: 0.2\nexperimentally great honor welcome president obama administer theresa may today offer condolences terrorist attack london strong well\n----- diversity: 0.2\ndeterminate many years talk action congress make american people thing russia coverage amp deduction amp replace obama administer theresa may today offer condolences terrorist attack london strong well\n"
    }
   ],
   "source": [
    "nb_samples = 5\n",
    "\n",
    "params = {\n",
    "    'maxlen': maxlen,\n",
    "    'vocab': nb_vocab,\n",
    "    'use_embeddings': True\n",
    "    }\n",
    "\n",
    "for _ in range(nb_samples):\n",
    "    data_test = choice(init_chars)\n",
    "\n",
    "    # Cuanto mas se acerque diversity a 1, menos sentido tendran los tweets\n",
    "    for diversity in [0.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "        sentence = copy(data_test)\n",
    "        generated = copy(data_test)\n",
    "        for i in range(len(data_test), 400):\n",
    "            x_pred = np.zeros((1, params['maxlen']))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
    "            preds = lm_model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample_pred(preds, diversity)\n",
    "            next_char = id2w[next_index]\n",
    "            if next_char == '<EOS>':\n",
    "                break\n",
    "            generated += [next_char]\n",
    "            sentence = sentence[1:] \n",
    "            sentence += [next_char]\n",
    "        tweet = ''.join(generated)\n",
    "        print(tweet[5:])"
   ]
  }
 ]
}