{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittfgpuconda4815d6332dd1431691508f462cd4b2ad",
   "display_name": "Python 3.7.6 64-bit ('tf-gpu': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3 Tweet Generation\n",
    "Ya se hizo la limpieza de datos inicial de este dataset en el notebook [Analisis Dataset 2](00_2_AnalisisDataset2.ipynb)\n",
    "\n",
    "En este notebook generaremos tweets haciendo que se parezcan a los que twitea Trump, para ello, usaremos Language Modeling con Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (2.2.5)\nRequirement already satisfied: spacy>=2.2.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.3)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\nRequirement already satisfied: setuptools in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.1.0.post20200127)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\nRequirement already satisfied: thinc<7.4.0,>=7.3.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\nRequirement already satisfied: srsly<1.1.0,>=0.1.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.1)\nRequirement already satisfied: numpy>=1.15.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\nRequirement already satisfied: idna<2.9,>=2.5 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.42.1)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\nRequirement already satisfied: zipp>=0.5 in d:\\programas\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.1.0)\n✔ Download and installation successful\nYou can now load the model via spacy.load('en_core_web_sm')\nc:\\Users\\asens\\.vscode\\extensions\\ms-python.python-2020.2.64397\\pythonFiles\\lib\\python\\past\\types\\oldstr.py:36: DeprecationWarning: invalid escape sequence \\d\n  \"\"\"\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\h5py\\_hl\\base.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import (Mapping, MutableMapping, KeysView,\nUsing TensorFlow backend.\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\learn_io\\generator_io.py:26: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  from collections import Container\n"
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import io\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import pylab as pl\n",
    "from random import shuffle, choice, sample\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from copy import copy\n",
    "from IPython import display\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dot, Concatenate, Flatten, Permute, Multiply, dot, concatenate\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "\n",
    "# Comando nvidia-smi para ver la lista de GPU\n",
    "\n",
    "set_session(tf.Session(config=config))  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permitir ver columnas completas\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Función para cargar dataframe\n",
    "def load_data(ruta, nombre):\n",
    "    path = os.path.join(ruta, nombre)    \n",
    "    return pd.read_csv(path, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>president</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>OBAMA</td>\n      <td>low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OBAMA</td>\n      <td>low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>OBAMA</td>\n      <td>national gun violence awareness day show commitment keep kid safe gun violence everyone weve lose take action change leadership laws reflect commitment matter long take</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>OBAMA</td>\n      <td>never truly debt owe fall heroes remember honor sacrifice live endure ideals justice equality oppounity generations americans give last full measure devotion</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>OBAMA</td>\n      <td>center leaders tomorrow ready step build world michelle grateful chicago city council make happen</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  president  \\\n0     OBAMA   \n1     OBAMA   \n2     OBAMA   \n3     OBAMA   \n4     OBAMA   \n\n                                                                                                                                                                        tweet  \n0  low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss  \n1  low plastic stool cheap delicious noodles cold hanoi beer ill remember tony teach us food impoantly ability bring us together make us little less afraid unknown well miss  \n2    national gun violence awareness day show commitment keep kid safe gun violence everyone weve lose take action change leadership laws reflect commitment matter long take  \n3               never truly debt owe fall heroes remember honor sacrifice live endure ideals justice equality oppounity generations americans give last full measure devotion  \n4                                                                           center leaders tomorrow ready step build world michelle grateful chicago city council make happen  "
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "df = load_data('data', 'dataset_2_limpieza_inicial.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionamos los tweets de Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>president</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2892</th>\n      <td>TRUMP</td>\n      <td>thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes</td>\n    </tr>\n    <tr>\n      <th>2893</th>\n      <td>TRUMP</td>\n      <td>thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes</td>\n    </tr>\n    <tr>\n      <th>2894</th>\n      <td>TRUMP</td>\n      <td>head canada grinÃ talk mostly center long time unfair trade unite state go singapore talk noh korea denuclearization wont talk russian witch hunt hoax</td>\n    </tr>\n    <tr>\n      <th>2895</th>\n      <td>TRUMP</td>\n      <td>congratulations washington capitals great play win cup championship alex team spectacular true dc many ways time</td>\n    </tr>\n    <tr>\n      <th>2896</th>\n      <td>TRUMP</td>\n      <td>look forward unfair trade deal grinÃ countries doesnt happen come even better</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5874</th>\n      <td>TRUMP</td>\n      <td>vast money nato amp unite state must pay powerful expensive defense provide germany</td>\n    </tr>\n    <tr>\n      <th>5875</th>\n      <td>TRUMP</td>\n      <td>despite hear fake news great meet chancellor angela merkel germany</td>\n    </tr>\n    <tr>\n      <th>5876</th>\n      <td>TRUMP</td>\n      <td>great meet committee morning</td>\n    </tr>\n    <tr>\n      <th>5877</th>\n      <td>TRUMP</td>\n      <td>president change small confidence</td>\n    </tr>\n    <tr>\n      <th>5878</th>\n      <td>TRUMP</td>\n      <td>noh korea behave badly play unite state years china do little help</td>\n    </tr>\n  </tbody>\n</table>\n<p>2987 rows × 2 columns</p>\n</div>",
      "text/plain": "     president  \\\n2892     TRUMP   \n2893     TRUMP   \n2894     TRUMP   \n2895     TRUMP   \n2896     TRUMP   \n...        ...   \n5874     TRUMP   \n5875     TRUMP   \n5876     TRUMP   \n5877     TRUMP   \n5878     TRUMP   \n\n                                                                                                                                                        tweet  \n2892                                                                   thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes  \n2893                                                                   thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes  \n2894  head canada grinÃ talk mostly center long time unfair trade unite state go singapore talk noh korea denuclearization wont talk russian witch hunt hoax  \n2895                                         congratulations washington capitals great play win cup championship alex team spectacular true dc many ways time  \n2896                                                                           look forward unfair trade deal grinÃ countries doesnt happen come even better  \n...                                                                                                                                                       ...  \n5874                                                                      vast money nato amp unite state must pay powerful expensive defense provide germany  \n5875                                                                                       despite hear fake news great meet chancellor angela merkel germany  \n5876                                                                                                                             great meet committee morning  \n5877                                                                                                                        president change small confidence  \n5878                                                                                       noh korea behave badly play unite state years china do little help  \n\n[2987 rows x 2 columns]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['president'] == 'TRUMP']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2987,\n 'thoughts prayers families serviceman kill fellow servicemen wound somalia truly heroes')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_dataset = list()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row[1]\n",
    "    trump_dataset.append(sentence)\n",
    "\n",
    "len(trump_dataset), trump_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar a nivel de carácter en lugar de palaba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [list(x) for x in trump_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['<SOS>', 't', 'h', 'o', 'u']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_chars = [x[:5] for x in tokenized] # me quedo con los 4 caracteres iniciales\n",
    "\n",
    "for i in range(len(init_chars)):\n",
    "    tmp = init_chars[i]\n",
    "    tmp.insert(0, '<SOS>')\n",
    "    init_chars[i] = tmp[:5]\n",
    "    \n",
    "init_chars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengo la longitud máxima de las frases y la longitud media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Longitud maxima:  219\nMedia de Longitud:  89.00836960160696\n"
    }
   ],
   "source": [
    "maxlen = max([len(x) for x in tokenized])\n",
    "avglen = sum([len(x) for x in tokenized])/len(tokenized)\n",
    "print('Longitud maxima: ', maxlen)\n",
    "print('Media de Longitud: ', avglen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Numero de tokens: 265868\n"
    }
   ],
   "source": [
    "total_tokens = [t for s in trump_dataset for t in s]\n",
    "print('Numero de tokens: {}'.format(len(total_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "34"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab_counter = Counter(total_tokens)\n",
    "vocab = [w for w, v in vocab_counter.items() if v > 2]  # Caracteres que mínimo aparezcan 2 veces\n",
    "vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + vocab\n",
    "\n",
    "nb_vocab = len(vocab)\n",
    "nb_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2id = {k:i for i, k in enumerate(vocab)}\n",
    "id2w = {i:k for k, i in c2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "256907"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 5\n",
    "\n",
    "step = 1\n",
    "\n",
    "data_train = []\n",
    "\n",
    "for x in tokenized:\n",
    "    x.insert(0, '<SOS>')\n",
    "    x.append('<EOS>')\n",
    "    for i in range(0, len(x)-maxlen, step):\n",
    "        data_train.append((x[i:i+maxlen], x[i+maxlen]))\n",
    "        \n",
    "len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pred(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampletest(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % SAMPLE_EVERY == 0  and epoch>0:\n",
    "            data_test = []\n",
    "            nb_samples = 1\n",
    "            \n",
    "            params = {\n",
    "                'maxlen': maxlen,\n",
    "                'vocab': nb_vocab,\n",
    "                'use_embeddings': True\n",
    "                }\n",
    "            for _ in range(nb_samples):\n",
    "                data_test = choice(init_chars)\n",
    "                for diversity in [0.2, 0.6, 1.2]:\n",
    "                    print('----- diversity:', diversity)\n",
    "                    sentence = copy(data_test)\n",
    "                    generated = copy(data_test)\n",
    "                    for i in range(len(data_test), 400):\n",
    "                        x_pred = np.zeros((1, params['maxlen']))\n",
    "                        for t, char in enumerate(sentence):\n",
    "                            x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
    "                        preds = self.model.predict(x_pred, verbose=0)[0]\n",
    "                        next_index = sample_pred(preds, diversity)\n",
    "                        next_char = id2w[next_index]\n",
    "                        if next_char == '<EOS>':\n",
    "                            break\n",
    "                        generated += [next_char]\n",
    "                        sentence = sentence[1:] \n",
    "                        sentence += [next_char]\n",
    "                    print(''.join(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDisplay(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.epochs = []\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        #plt.show()\n",
    "        \n",
    "        plt.ion()\n",
    "        self.fig.show()\n",
    "        self.fig.canvas.draw()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.epochs.append(epoch)\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.accs.append(logs['acc'])\n",
    "        if epoch % PLOT_EVERY == 0:\n",
    "            \n",
    "            self.ax.clear()\n",
    "            self.ax.plot(self.epochs, self.accs, 'g', label='acc')\n",
    "            self.ax.plot(self.epochs, self.losses, 'b', label='loss')\n",
    "            legend = self.ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n",
    "            #display.clear_output(wait=True)\n",
    "            #display.display(pl.gcf())\n",
    "            self.fig.canvas.draw()\n",
    "            \n",
    "            #plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decidir arquitectura y preparar el train y el predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs.pop('params', None)\n",
    "    \n",
    "    def compile_bidirectional(self, params={}):\n",
    "        lm_inputs = Input(shape=(params['maxlen'], ), name='lm_input')\n",
    "        embeddings = Embedding(params['vocab'], params['emb_feats'])(lm_inputs)\n",
    "        lstm =  CuDNNLSTM(params['rnn_hidden'], return_sequences=True, name='rnn1')        \n",
    "        \n",
    "        lmlstm = Bidirectional(lstm)(embeddings)       \n",
    "        \n",
    "        stacklstm =  CuDNNLSTM(params['rnn_hidden'], return_sequences=False, name='stacked')\n",
    "        stackedlstm = stacklstm(lmlstm)\n",
    "        \n",
    "        lmout = Dense(params['vocab'], activation='softmax')(stackedlstm)\n",
    "        \n",
    "        model = Model(lm_inputs, lmout)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='rmsprop', \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, model, data, params={}):\n",
    "        callbacks = self._get_callbacks()\n",
    "        \n",
    "        if 'shuffle' in params and params['shuffle']:\n",
    "            shuffle(data)\n",
    "            \n",
    "        sentences, next_chars = zip(*data)\n",
    "        #print(sentences[0])\n",
    "\n",
    "        x = np.zeros((len(data), params['maxlen']))\n",
    "        y = np.zeros((len(data), params['vocab']))\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[i, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
    "            y[i, c2id[next_chars[i]] if next_chars[i] in c2id else c2id['<UNK>']]  = 1\n",
    "        \n",
    "        model.fit(x, y, batch_size=params['batch_size'], epochs=params['epochs'], callbacks=callbacks, verbose=1)\n",
    "\n",
    "    def predict(self, model, data, params={}):        \n",
    "        if 'use_embeddings' in params and params['use_embeddings']:\n",
    "            # variedad en las predicciones\n",
    "            for diversity in [0.2, 0.6, 1.2]:\n",
    "                print('----- diversity:', diversity)\n",
    "                sentence = copy(data)\n",
    "                generated = copy(data)\n",
    "                # cuantas predicciones queremos hacer\n",
    "                for i in range(len(data), 400):\n",
    "                    x_pred = np.zeros((1, params['maxlen']))\n",
    "                    # preparar inpunt\n",
    "                    for t, char in enumerate(sentence):\n",
    "                        x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
    "                    # predecir\n",
    "                    preds = self.model.predict(x_pred, verbose=0)[0]\n",
    "                    next_index = sample_pred(preds, diversity)\n",
    "                    next_char = id2w[next_index]\n",
    "                    # mirar si hemos terminado\n",
    "                    if next_char == '<EOS>':\n",
    "                        break\n",
    "                    # ana                        \n",
    "                    generated += [next_char]\n",
    "                    sentence = sentence[1:] \n",
    "                    sentence += [next_char]\n",
    "                print(''.join(generated))\n",
    "    \n",
    "    \n",
    "    def load(self, model_path='seq2seq_attn.h5'):\n",
    "        return load_model(model_path)\n",
    "    \n",
    "    def _get_callbacks(self, model_path='seq2seq_attn.h5'):\n",
    "        es = EarlyStopping(monitor='loss', patience=4, mode='auto', verbose=0)       \n",
    "        \n",
    "        save_best = ModelCheckpoint(model_path, monitor='loss', verbose = 0, save_best_only=True, save_weights_only=False, period=2)\n",
    "        st = Sampletest()\n",
    "        # hd = HistoryDisplay()\n",
    "        rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
    "        return [st, rlr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_params = {\n",
    "    'maxlen': maxlen, \n",
    "    'vocab': len(vocab),\n",
    "    'emb_feats': 100,\n",
    "    'rnn_hidden': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From D:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From D:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From D:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nD:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:339: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  if not isinstance(values, collections.Sequence):\nWARNING:tensorflow:From D:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From D:\\Programas\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlm_input (InputLayer)        (None, 5)                 0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 5, 100)            3400      \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 5, 512)            733184    \n_________________________________________________________________\nstacked (CuDNNLSTM)          (None, 256)               788480    \n_________________________________________________________________\ndense_1 (Dense)              (None, 34)                8738      \n=================================================================\nTotal params: 1,533,802\nTrainable params: 1,533,802\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "lm = LM()\n",
    "lm_model = lm.compile_bidirectional(params=compile_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}